# Attention Mechanisms Library

[å¤§è¯­è¨€æ¨¡å‹æ³¨æ„åŠ›æœºåˆ¶è¯¦è§£](https://vistart.github.io/examples/llm/attention_mechanism_comparison.html#gqa)

[English](#english) | [ä¸­æ–‡](#ä¸­æ–‡)

## English

A comprehensive PyTorch implementation of various attention mechanisms used in modern Transformer architectures. This library provides educational and practical implementations of state-of-the-art attention variants, from standard multi-head attention to efficient alternatives like Flash Attention, Linear Attention, and Sparse Attention.

### ğŸ¯ Features

- **Complete Implementations**: 15+ different attention mechanisms with detailed comments
- **Performance Optimized**: Memory-efficient implementations with benchmarking tools
- **Easy to Use**: Unified interface and factory pattern for easy switching between attention types
- **Educational**: Well-documented code with theoretical explanations
- **Production Ready**: Includes caching, position encodings, and optimization techniques

### ğŸ“‹ Requirements

```bash
torch>=1.10.0
numpy>=1.19.0
```

### ğŸš€ Quick Start

```python
from main import AttentionFactory, AttentionConfig

# Create a configuration
config = AttentionConfig(
    attention_type="flash",  # Options: full, flash, linear, sparse, gqa, mqa
    d_model=512,
    n_heads=8,
    dropout=0.1
)

# Create attention module
attention = AttentionFactory.create_attention(config)

# Use in your model
import torch
x = torch.randn(2, 128, 512)  # [batch_size, seq_len, d_model]
output = attention(x)
```

### ğŸ“ Project Structure

```
attention-mechanisms/
â”œâ”€â”€ full_attention_code.py      # Standard multi-head attention
â”œâ”€â”€ flash_attention_code.py     # Flash Attention & Flash Attention v2
â”œâ”€â”€ linear_attention_code.py    # Linear complexity attention variants
â”œâ”€â”€ sparse_attention_code.py    # Sparse attention patterns
â”œâ”€â”€ gqa_mqa_code.py             # Grouped Query & Multi-Query Attention
â””â”€â”€ main.py                     # Main interface and examples
```

### ğŸ”§ Attention Mechanisms

#### 1. **Standard Attention** (`full_attention_code.py`)
- Multi-Head Attention (MHA)
- Self-Attention
- Cross-Attention
- Scaled Dot-Product Attention

#### 2. **Flash Attention** (`flash_attention_code.py`)
- Flash Attention v1 & v2
- Block-wise computation for memory efficiency
- Support for causal masks and ALiBi position encoding
- O(NÂ²) time, O(N) memory complexity

#### 3. **Linear Attention** (`linear_attention_code.py`)
- **Linear Attention**: Basic linear attention with feature maps
- **Performer**: Random feature approximation of softmax kernel
- **Linformer**: Low-rank projection of keys and values
- **NystrÃ¶m**: NystrÃ¶m approximation of attention matrix
- **Cosformer**: Cosine-based reweighting
- O(N) complexity for long sequences

#### 4. **Sparse Attention** (`sparse_attention_code.py`)
- **Local/Sliding Window**: Fixed window attention
- **Block Sparse**: Block-diagonal patterns
- **Strided/Dilated**: Strided attention patterns
- **Longformer**: Combination of local and global attention
- **BigBird**: Random, local, and global attention

#### 5. **Memory-Efficient Attention** (`gqa_mqa_code.py`)
- **Grouped Query Attention (GQA)**: Groups of queries share key-value pairs
- **Multi-Query Attention (MQA)**: All queries share single key-value pair
- Significantly reduces KV-cache memory during inference

### ğŸ’¡ Usage Examples

#### Scenario-Based Selection

```python
from main import AttentionSelector

# Automatically select best attention for your scenario
config = AttentionSelector.select_for_scenario(
    scenario="long_context",  # Options: training, inference, long_context, memory_limited
    seq_len=8192,
    memory_constraint=2000  # MB
)
```

#### Custom Model with Mixed Attention

```python
from main import TransformerBlock, AttentionConfig

# Create a model with different attention types per layer
configs = [
    AttentionConfig(attention_type="local", window_size=128),     # Bottom layers
    AttentionConfig(attention_type="block_sparse", block_size=64), # Middle layers  
    AttentionConfig(attention_type="full")                         # Top layers
]

layers = [TransformerBlock(config) for config in configs]
```

#### Performance Benchmarking

```python
from main import AttentionProfiler

# Profile different attention mechanisms
profile = AttentionProfiler.profile_attention(
    attention_type="flash",
    batch_size=4,
    seq_len=2048,
    num_iterations=100
)
print(f"Latency: {profile['avg_latency_ms']:.2f}ms")
print(f"Memory: {profile['memory_mb']:.2f}MB")
```

### ğŸ“Š Performance Comparison

| Attention Type | Time Complexity | Memory Complexity | Best For |
|---|---|---|---|
| Standard MHA | O(NÂ²d) | O(NÂ²) | Short sequences (<1K) |
| Flash Attention | O(NÂ²d) | O(N) | Training with long sequences |
| Linear Attention | O(NdÂ²) | O(Nd) | Very long sequences (>8K) |
| Local Attention | O(Nwd) | O(Nw) | Long sequences with local patterns |
| GQA/MQA | O(NÂ²d) | O(NÂ²/g) | Inference optimization |

### ğŸ“ Educational Resources

Each implementation includes:
- Detailed docstrings explaining the algorithm
- Time and space complexity analysis
- References to original papers
- Practical usage examples

### ğŸ” Advanced Features

- **Position Encodings**: RoPE, ALiBi, xPos support
- **KV Caching**: For efficient autoregressive generation
- **Mixed Precision**: Compatible with fp16/bf16 training
- **Custom Masks**: Support for arbitrary attention patterns

### ğŸ“ License

[MIT License](LICENSE)

---

## ä¸­æ–‡

ä¸€ä¸ªå…¨é¢çš„PyTorchæ³¨æ„åŠ›æœºåˆ¶å®ç°åº“ï¼ŒåŒ…å«ç°ä»£Transformeræ¶æ„ä¸­ä½¿ç”¨çš„å„ç§æ³¨æ„åŠ›å˜ä½“ã€‚æœ¬åº“æä¾›äº†ä»æ ‡å‡†å¤šå¤´æ³¨æ„åŠ›åˆ°Flash Attentionã€çº¿æ€§æ³¨æ„åŠ›å’Œç¨€ç–æ³¨æ„åŠ›ç­‰é«˜æ•ˆæ›¿ä»£æ–¹æ¡ˆçš„æ•™è‚²æ€§å’Œå®ç”¨æ€§å®ç°ã€‚

### ğŸ¯ ä¸»è¦ç‰¹æ€§

- **å®Œæ•´å®ç°**ï¼š15+ç§ä¸åŒçš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¸¦è¯¦ç»†æ³¨é‡Š
- **æ€§èƒ½ä¼˜åŒ–**ï¼šå†…å­˜é«˜æ•ˆçš„å®ç°ï¼ŒåŒ…å«åŸºå‡†æµ‹è¯•å·¥å…·
- **æ˜“äºä½¿ç”¨**ï¼šç»Ÿä¸€æ¥å£å’Œå·¥å‚æ¨¡å¼ï¼Œä¾¿äºåœ¨ä¸åŒæ³¨æ„åŠ›ç±»å‹é—´åˆ‡æ¢
- **æ•™è‚²æ€§å¼º**ï¼šä»£ç æ–‡æ¡£å®Œå–„ï¼ŒåŒ…å«ç†è®ºè§£é‡Š
- **ç”Ÿäº§å°±ç»ª**ï¼šåŒ…å«ç¼“å­˜ã€ä½ç½®ç¼–ç å’Œä¼˜åŒ–æŠ€æœ¯

### ğŸ“‹ ç¯å¢ƒè¦æ±‚

```bash
torch>=1.10.0
numpy>=1.19.0
```

### ğŸš€ å¿«é€Ÿå¼€å§‹

```python
from main import AttentionFactory, AttentionConfig

# åˆ›å»ºé…ç½®
config = AttentionConfig(
    attention_type="flash",  # é€‰é¡¹ï¼šfull, flash, linear, sparse, gqa, mqa
    d_model=512,
    n_heads=8,
    dropout=0.1
)

# åˆ›å»ºæ³¨æ„åŠ›æ¨¡å—
attention = AttentionFactory.create_attention(config)

# åœ¨æ¨¡å‹ä¸­ä½¿ç”¨
import torch
x = torch.randn(2, 128, 512)  # [æ‰¹æ¬¡å¤§å°, åºåˆ—é•¿åº¦, æ¨¡å‹ç»´åº¦]
output = attention(x)
```

### ğŸ“ é¡¹ç›®ç»“æ„

```
attention-mechanisms/
â”œâ”€â”€ full_attention_code.py      # æ ‡å‡†å¤šå¤´æ³¨æ„åŠ›
â”œâ”€â”€ flash_attention_code.py     # Flash AttentionåŠv2ç‰ˆæœ¬
â”œâ”€â”€ linear_attention_code.py    # çº¿æ€§å¤æ‚åº¦æ³¨æ„åŠ›å˜ä½“
â”œâ”€â”€ sparse_attention_code.py    # ç¨€ç–æ³¨æ„åŠ›æ¨¡å¼
â”œâ”€â”€ gqa_mqa_code.py             # åˆ†ç»„æŸ¥è¯¢å’Œå¤šæŸ¥è¯¢æ³¨æ„åŠ›
â””â”€â”€ main.py                     # ä¸»æ¥å£å’Œç¤ºä¾‹
```

### ğŸ”§ æ³¨æ„åŠ›æœºåˆ¶è¯¦è§£

#### 1. **æ ‡å‡†æ³¨æ„åŠ›** (`full_attention_code.py`)
- å¤šå¤´æ³¨æ„åŠ›ï¼ˆMHAï¼‰
- è‡ªæ³¨æ„åŠ›
- äº¤å‰æ³¨æ„åŠ›
- ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›

#### 2. **Flashæ³¨æ„åŠ›** (`flash_attention_code.py`)
- Flash Attention v1å’Œv2
- åˆ†å—è®¡ç®—ä»¥æé«˜å†…å­˜æ•ˆç‡
- æ”¯æŒå› æœæ©ç å’ŒALiBiä½ç½®ç¼–ç 
- O(NÂ²)æ—¶é—´å¤æ‚åº¦ï¼ŒO(N)å†…å­˜å¤æ‚åº¦

#### 3. **çº¿æ€§æ³¨æ„åŠ›** (`linear_attention_code.py`)
- **çº¿æ€§æ³¨æ„åŠ›**ï¼šä½¿ç”¨ç‰¹å¾æ˜ å°„çš„åŸºç¡€çº¿æ€§æ³¨æ„åŠ›
- **Performer**ï¼šè½¯æœ€å¤§æ ¸çš„éšæœºç‰¹å¾è¿‘ä¼¼
- **Linformer**ï¼šé”®å€¼çš„ä½ç§©æŠ•å½±
- **NystrÃ¶m**ï¼šæ³¨æ„åŠ›çŸ©é˜µçš„NystrÃ¶mè¿‘ä¼¼
- **Cosformer**ï¼šåŸºäºä½™å¼¦çš„é‡åŠ æƒ
- é•¿åºåˆ—çš„O(N)å¤æ‚åº¦

#### 4. **ç¨€ç–æ³¨æ„åŠ›** (`sparse_attention_code.py`)
- **å±€éƒ¨/æ»‘åŠ¨çª—å£**ï¼šå›ºå®šçª—å£æ³¨æ„åŠ›
- **å—ç¨€ç–**ï¼šå—å¯¹è§’æ¨¡å¼
- **è·¨æ­¥/ç¨€é‡Š**ï¼šè·¨æ­¥æ³¨æ„åŠ›æ¨¡å¼
- **Longformer**ï¼šå±€éƒ¨å’Œå…¨å±€æ³¨æ„åŠ›çš„ç»„åˆ
- **BigBird**ï¼šéšæœºã€å±€éƒ¨å’Œå…¨å±€æ³¨æ„åŠ›

#### 5. **å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›** (`gqa_mqa_code.py`)
- **åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGQAï¼‰**ï¼šæŸ¥è¯¢ç»„å…±äº«é”®å€¼å¯¹
- **å¤šæŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆMQAï¼‰**ï¼šæ‰€æœ‰æŸ¥è¯¢å…±äº«å•ä¸ªé”®å€¼å¯¹
- æ˜¾è‘—å‡å°‘æ¨ç†æ—¶çš„KVç¼“å­˜å†…å­˜

### ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹

#### åŸºäºåœºæ™¯çš„è‡ªåŠ¨é€‰æ‹©

```python
from main import AttentionSelector

# æ ¹æ®åœºæ™¯è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ³¨æ„åŠ›
config = AttentionSelector.select_for_scenario(
    scenario="long_context",  # é€‰é¡¹ï¼štraining, inference, long_context, memory_limited
    seq_len=8192,
    memory_constraint=2000  # MB
)
```

#### æ··åˆæ³¨æ„åŠ›çš„è‡ªå®šä¹‰æ¨¡å‹

```python
from main import TransformerBlock, AttentionConfig

# åˆ›å»ºæ¯å±‚ä½¿ç”¨ä¸åŒæ³¨æ„åŠ›ç±»å‹çš„æ¨¡å‹
configs = [
    AttentionConfig(attention_type="local", window_size=128),     # åº•å±‚
    AttentionConfig(attention_type="block_sparse", block_size=64), # ä¸­å±‚
    AttentionConfig(attention_type="full")                         # é¡¶å±‚
]

layers = [TransformerBlock(config) for config in configs]
```

#### æ€§èƒ½åŸºå‡†æµ‹è¯•

```python
from main import AttentionProfiler

# åˆ†æä¸åŒæ³¨æ„åŠ›æœºåˆ¶çš„æ€§èƒ½
profile = AttentionProfiler.profile_attention(
    attention_type="flash",
    batch_size=4,
    seq_len=2048,
    num_iterations=100
)
print(f"å»¶è¿Ÿ: {profile['avg_latency_ms']:.2f}ms")
print(f"å†…å­˜: {profile['memory_mb']:.2f}MB")
```

### ğŸ“Š æ€§èƒ½å¯¹æ¯”

| æ³¨æ„åŠ›ç±»å‹ | æ—¶é—´å¤æ‚åº¦ | å†…å­˜å¤æ‚åº¦ | æœ€é€‚ç”¨äº |
|---|---|---|---|
| æ ‡å‡†MHA | O(NÂ²d) | O(NÂ²) | çŸ­åºåˆ—ï¼ˆ<1Kï¼‰ |
| Flashæ³¨æ„åŠ› | O(NÂ²d) | O(N) | é•¿åºåˆ—è®­ç»ƒ |
| çº¿æ€§æ³¨æ„åŠ› | O(NdÂ²) | O(Nd) | è¶…é•¿åºåˆ—ï¼ˆ>8Kï¼‰ |
| å±€éƒ¨æ³¨æ„åŠ› | O(Nwd) | O(Nw) | å…·æœ‰å±€éƒ¨æ¨¡å¼çš„é•¿åºåˆ— |
| GQA/MQA | O(NÂ²d) | O(NÂ²/g) | æ¨ç†ä¼˜åŒ– |

### ğŸ“ æ•™è‚²èµ„æº

æ¯ä¸ªå®ç°éƒ½åŒ…å«ï¼š
- è§£é‡Šç®—æ³•çš„è¯¦ç»†æ–‡æ¡£å­—ç¬¦ä¸²
- æ—¶é—´å’Œç©ºé—´å¤æ‚åº¦åˆ†æ
- åŸå§‹è®ºæ–‡å¼•ç”¨
- å®é™…ä½¿ç”¨ç¤ºä¾‹

### ğŸ” é«˜çº§åŠŸèƒ½

- **ä½ç½®ç¼–ç **ï¼šæ”¯æŒRoPEã€ALiBiã€xPos
- **KVç¼“å­˜**ï¼šç”¨äºé«˜æ•ˆçš„è‡ªå›å½’ç”Ÿæˆ
- **æ··åˆç²¾åº¦**ï¼šå…¼å®¹fp16/bf16è®­ç»ƒ
- **è‡ªå®šä¹‰æ©ç **ï¼šæ”¯æŒä»»æ„æ³¨æ„åŠ›æ¨¡å¼

### ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤é—®é¢˜å’Œæ‹‰å–è¯·æ±‚ï¼

### ğŸ“ è®¸å¯è¯

[MIT License](LICENSE)